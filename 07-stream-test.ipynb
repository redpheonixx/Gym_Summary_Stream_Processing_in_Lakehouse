{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b142670-2829-41da-a5b1-0678d07ead46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "dbutils.widgets.text(\"Environment\", \"dev\", \"Set the current environment/catalog name\")\n",
    "dbutils.widgets.text(\"Host\", \"\", \"Databricks Workspace URL\")\n",
    "dbutils.widgets.text(\"AccessToken\", \"\", \"Secure Access Token\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "env = dbutils.widgets.get(\"Environment\")\n",
    "host = dbutils.widgets.get(\"Host\")\n",
    "token = dbutils.widgets.get(\"AccessToken\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./02-setup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "SH = SetupHelper(env)\n",
    "SH.cleanup()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "job_payload = \\\n",
    "{\n",
    "        \"name\": \"stream-test\",\n",
    "        \"webhook_notifications\": {},\n",
    "        \"timeout_seconds\": 0,\n",
    "        \"max_concurrent_runs\": 1,\n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"task_key\": \"stream-test-task\",\n",
    "                \"run_if\": \"ALL_SUCCESS\",\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Repos/SBIT/SBIT/07-run\",\n",
    "                    \"source\": \"WORKSPACE\"\n",
    "                },\n",
    "                \"job_cluster_key\": \"Job_cluster\",\n",
    "                \"timeout_seconds\": 0,\n",
    "                \"email_notifications\": {}\n",
    "            }\n",
    "        ],\n",
    "        \"job_clusters\": [\n",
    "            {\n",
    "                \"job_cluster_key\": \"Job_cluster\",\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                    \"spark_conf\": {\n",
    "                        \"spark.databricks.delta.preview.enabled\": \"true\",\n",
    "                        \"spark.master\": \"local[*, 4]\",\n",
    "                        \"spark.databricks.cluster.profile\": \"singleNode\"\n",
    "                    },\n",
    "                    \"azure_attributes\": {\n",
    "                        \"first_on_demand\": 1,\n",
    "                        \"availability\": \"ON_DEMAND_AZURE\",\n",
    "                        \"spot_bid_max_price\": -1\n",
    "                    },\n",
    "                    \"node_type_id\": \"Standard_DS4_v2\",\n",
    "                    \"driver_node_type_id\": \"Standard_DS4_v2\",\n",
    "                    \"custom_tags\": {\n",
    "                        \"ResourceClass\": \"SingleNode\"\n",
    "                    },\n",
    "                    \"data_security_mode\": \"SINGLE_USER\",\n",
    "                    \"runtime_engine\": \"STANDARD\",\n",
    "                    \"num_workers\": 0\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"format\": \"MULTI_TASK\"\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create a streaming job\n",
    "import requests\n",
    "import json\n",
    "create_response = requests.post(host + '/api/2.1/jobs/create', data=json.dumps(job_payload), auth=(\"token\", token))\n",
    "print(f\"Response: {create_response}\")\n",
    "job_id = json.loads(create_response.content.decode('utf-8'))[\"job_id\"]\n",
    "print(f\"Created Job {job_id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Trigger the streaming job\n",
    "run_payload = {\"job_id\": job_id, \"notebook_params\": {\"Environment\":env, \"RunType\": \"stream\", \"ProcessingTime\": \"1 seconds\"}}\n",
    "run_response = requests.post(host + '/api/2.1/jobs/run-now', data=json.dumps(run_payload), auth=(\"token\", token))\n",
    "run_id = json.loads(run_response.content.decode('utf-8'))[\"run_id\"]\n",
    "print(f\"Started Job run {run_id}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Wait until job starts\n",
    "import time\n",
    "status_payload = {\"run_id\": run_id}\n",
    "job_status=\"PENDING\"\n",
    "while job_status == \"PENDING\":\n",
    "    time.sleep(20)\n",
    "    status_job_response = requests.get(host + '/api/2.1/jobs/runs/get', data=json.dumps(status_payload), auth=(\"token\", token))\n",
    "    job_status = json.loads(status_job_response.content.decode('utf-8'))[\"tasks\"][0][\"state\"][\"life_cycle_state\"]  \n",
    "    print(job_status)    \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./03-history-loader\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./10-producer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./04-bronze\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./05-silver\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./06-gold\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Sleep for 2 minutes and let setup and history loader finish...\")\n",
    "time.sleep(2*60)\n",
    "\n",
    "#Validate setup and history load\n",
    "HL = HistoryLoader(env)\n",
    "PR = Producer()\n",
    "BZ = Bronze(env)\n",
    "SL = Silver(env)\n",
    "GL = Gold(env)\n",
    "\n",
    "SH.validate()\n",
    "HL.validate()\n",
    "\n",
    "#Produce some incremantal\n",
    "PR.produce(1)\n",
    "PR.validate(1)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Sleep for 2 minutes and let microbatch pickup the data...\")\n",
    "time.sleep(2*60)\n",
    "\n",
    "#Validate bronze, silver and gold layer \n",
    "BZ.validate(1)\n",
    "SL.validate(1)\n",
    "GL.validate(1)\n",
    " \n",
    "\n",
    "#Produce some incremantal data and wait for micro batch\n",
    "PR.produce(2)\n",
    "PR.validate(2)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Sleep for 2 minutes and let microbatch pickup the data...\")\n",
    "time.sleep(2*60)\n",
    "\n",
    "#Validate bronze, silver and gold layer \n",
    "BZ.validate(2)\n",
    "SL.validate(2)\n",
    "GL.validate(2)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Terminate the streaming Job\n",
    "cancel_payload = {\"run_id\": run_id}\n",
    "cancel_response = requests.post(host + '/api/2.1/jobs/runs/cancel', data=json.dumps(cancel_payload), auth=(\"token\", token))\n",
    "print(f\"Canceled Job run {run_id}. Status {cancel_response}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Delete the Job\n",
    "delete_job_payload = {\"job_id\": job_id}\n",
    "delete_job_response = requests.post(host + '/api/2.1/jobs/delete', data=json.dumps(delete_job_payload), auth=(\"token\", token))\n",
    "print(f\"Canceled Job run {run_id}. Status {delete_job_response}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbutils.notebook.exit(\"SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07-stream-test",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
